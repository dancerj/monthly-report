%; whizzy chapter -dvi
% -initex iniptex -latex platex -format platex -bibtex jbibtex -fmt fmt
% 以上 whizzytex を使用する場合の設定。

%     Tokyo Debian Meeting resources
%     Copyright (C) 2012 Junichi Uekawa
%     Copyright (C) 2011, 2015, 2020 Nobuhiro Iwamatsu

%     This program is free software; you can redistribute it and/or modify
%     it under the terms of the GNU General Public License as published by
%     the Free Software Foundation; either version 2 of the License, or
%     (at your option) any later version.

%     This program is distributed in the hope that it will be useful,
%     but WITHOUT ANY WARRANTY; without even the implied warranty of
%     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%     GNU General Public License for more details.

%     You should have received a copy of the GNU General Public License
%     along with this program; if not, write to the Free Software
%     Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

%  preview (shell-command (concat "evince " (replace-regexp-in-string "tex$" "pdf"(buffer-file-name)) "&"))

%%ここからヘッダ開始。

\documentclass[mingoth,a4paper]{jsarticle}
\usepackage{monthlyreport}
% 日付を定義する、毎月変わります。
\newcommand{\debmtgyear}{2020}
\newcommand{\debmtgmonth}{7}
\newcommand{\debmtgdate}{18}
% started from zero:
% (let ((year 2013) (month 7)) (+ (* (- year 2005) 12) month -1))
\newcommand{\debmtgnumber}{17}

% Needed to import pandoc-generated LaTeX documents.
% See https://stackoverflow.com/questions/40438037/tightlist-error-using-pandoc-with-markdown
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% tikz picture の為のマクロ設定
\usepackage[dvipdfmx]{graphicx}
\usepackage{tikz}

\begin{document}

\begin{titlepage}
\thispagestyle{empty}
% タイトルページ:編集必要な部分は最初のマクロに飛ばすこと

\vspace*{-2cm}
第\debmtgnumber{}回 東京エリア Debian 勉強会資料\\
\hspace*{-2cm}
\includegraphics{image2012-natsu/dotdeb.pdf}\\
\hfill{}\debmtgyear{}年\debmtgmonth{}月\debmtgdate{}日

% ここはアップデートすること
% 全角文字にしないとフォントのサイズが合わないので注意
\rotatebox{10}{\fontsize{30}{30} {\gt　ニューラルネットワーク特集}}\\

\vspace*{-2cm}
\hfill{}\includegraphics[height=6cm]{image200502/openlogo-nd.eps}
\end{titlepage}

\newpage

\begin{minipage}[b]{0.2\hsize}
 \definecolor{titleback}{gray}{0.9}
 \colorbox{titleback}{\rotatebox{90}{\fontsize{80}{80} {\gt デビアン勉強会} }}
\end{minipage}
\begin{minipage}[b]{0.8\hsize}
\hrule
\vspace{2mm}
\hrule
\begin{multicols}{2}
\tableofcontents
\end{multicols}
\vspace{2mm}
\hrule
\end{minipage}

\dancersection{最近のDebian関連のミーティング報告}{杉本　典充}

\subsection{2020年6月度 東京エリア・関西合同Debian勉強会}

2020年6月29日(土)に東京エリアDebian勉強会と関西Debian勉強会の合同でオンラインによるDebian勉強会を開催しました。参加者は12名でした。


\dancersection{事前課題}{杉本　典充}

今回の事前課題は以下です。

\begin{enumerate}
\item xxx
\end{enumerate}

%この課題に対して提出いただいた内容は以下です。

\begin{multicols}{2}
{\small
\input{image202007/prework.tex}
}
\end{multicols}

%\dancersection{Debian Trivia Quiz}{username}
%
%Debianの昨今の話題についてのQuizです。
%
%今回の出題範囲は\url{debian-devel-announce@lists.debian.org} や \url{debian-news@lists.debian.org}などに投稿された内容からです。
%
%\begin{multicols}{2}
%\input{image202007/quiz.tex}
%\end{multicols}


% % (query-replace-regexp "<.*?>" "")
% % (query-replace-regexp "^[    ]\+" "")

%\dancersection{titme}{username}
%-------------------------------------------------------------------------------

\dancersection{10年ぶりのニューラルネットワーク}{野首貴嗣}

過去、2010年3月に行われた第62回東京エリアDebian勉強会では
「ニューラルネットワークで画像認識してみた」
というタイトルで本庄弘典さんが発表をされていました。

それから10年が経過した2020年7月現在、ニューラルネットワークをとりまく
状況がどのように変化したかを俯瞰的に見ていきます。

また、個人的に開発、公開している成果につても紹介します。

\subsection{ニューラルネットワークのお気持ち}

まず、ニューラルネットワークについての厳密ではない、なんとなくなお気持ちで
概要を解説します。

パーセプトロン(\url{https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3})と呼ばれる、人間の脳(ニューロン)を模した演算ユニットがあります。
これは入力信号Xに対し、重みWを乗じバイアスbを加算した結果Yをえる単純なユニットです。さらに、Yに対し活性化関数と呼ばれる関数を適用しその結果を分類タスクの結果として考えます。

分類が正しいなら1を、間違っているなら0を返す関数と考え、実際に用意したデータをもちいて訓練を行います。訓練は、逆誤差伝播(バックプロパゲーション)という手法を使い、求められた結果と訓練データ上の正解との誤差を最小化するよう、重みを調整していきます。

単一のパーセプトロンはANDやORを表現することができます。パーセプトロンの式は線形分離境界を表していると考えることで直感的に理解できるでしょう。
さらには、XORは実現できないことも理解できると思います(1つの直線で分離できないため)。

ニューロンを増やしたり、多段に接続することで、より複雑な非線形分離が可能となります。この段数が概ね4段以上になったものをディープラーニングと呼ぶことが多いようです。

\subsection{ブレイクスルー}

2012年に、Googleが''Using large-scale brain simulations for machine learning and A.I.''(\url{https://blog.google/topics/machine-learning/using-large-scale-brain-simulations-for/})というblog記事と論文
(\url{http://static.googleusercontent.com/media/research.google.com/ja//archive/unsupervised_icml2012.pdf})
を公開しました。
16コアをもつマシン169台を駆使し、大量の画像を訓練することで教師なしで人体や猫の特徴を獲得したと話題になりました。このときに採用されたアーキテクチャはAutoAEncoderと呼ばれるものです。

AutoEncoderは入力側と出力側が等しくなるように構成された多段ニューラルネットワークで、入出力画像よりも小さなベクトル表現によって、入力画像の特徴を獲得することができるものです。
AutoEncoderが行っていることは、一種の次元圧縮ともいえます。ニューラルネットワークを用いない次元圧縮手法もありますが、多段ニューラルネットワーク(ディープラーニング)が実用になることを示した大きな一歩でした。

\subsection{ブレイクスルー実現要素}

従来のニューラルネットワークが抱えていた課題を解決した要素にはさまざまなものがあります。

\begin{description}
\item[計算資源の向上] \mbox{} \\
  私がニューラルネットワークに触れた1995年の時点で、実際に利用されていたコンピュータのスペックは次のようなものでした。
  \begin{itemize}
  \item PA-RISC 25Mhz
  \item メモリ16MB
  \end{itemize}
  そもそも利用可能なメモリが少なく、大きなニューラルネットワーク構造を持つこと自体が困難でした。現代においてはスマートフォンでも4GB程度のメモリを搭載するものも珍しくありません。
\item[GPGPUの一般化] \mbox{} \\
  CPU自体も当時と比較してかなり高速にはなりましたが、単一コアあたりの処理能力向上には限界が見えつつあります。一方でグラフィックス処理向けに設計されたGPUをより一般的なコンピューティングに応用する環境が整備されてきました。\\
  GPUは特に並列計算を得意としており、多段ニューラルネットワークの演算とは非常に相性がよく、その方向へ進化していっています。
\item[大規模データセットの拡充]  \mbox{} \\
  Googleの猫の例で活躍したのは、Google photosやFacebook album等、画像データを大量に収集できるプラットフォームの存在でした。\\
  さらに、ラベル(物体の種別情報)を付与したデータセットも増えていきます。これにはクラウドソーシングサービスの拡充(Amazon Mechanical Turk, Cloudworks等)も寄与しています。
\item[技術的な進歩]  \mbox{} \\
  多段ニューラルネットワークが抱えていた他の問題として、勾配消失/発散問題がありました。逆誤差伝播をする際に、層が増えることによって伝播すべき誤差がどんどん小さくなってしまったり、逆に大きくなったりする問題です。\\
  要因の一つに、活性化関数があります。シグモイド関数を使うことが昔は一般的でしたが、この関数が取りうる値は0〜1しかありません。より幅広い値を扱える活性化関数(ReLU等のランプ関数)の出現により、より大きな勾配を扱えるようになりました。\\
  また勾配のクリッピングによって、勾配爆発を抑えることができるようになりました。
  その他Batch Normalizationなどの正規化手法も学習の安定化に寄与しています。
\item[分散表現の活用]  \mbox{} \\
  単語や抽象的なカテゴリーといったものを表現するために分散表現(埋め込み表現)が活用できることがわかり、自然言語処理などへの活用が広がりました。word2vecは単語埋め込み表現の最たる活用例です。
\item[データ拡張]  \mbox{} \\
  既存のデータを変形したり、ノイズをのせるなどのデータ拡張技術が主に画像認識分野で発達しています。これにより、学習データを水増しすることができ、認識制度向上に寄与しています。
\item[教師データを必要としない学習]  \mbox{} \\
  一般的な教師あり学習には正解データを用意する必要があり、その作成にはコストがかかります。一方でword2vecのような、教師データを必要としない手法が発達してきています。\\
  既存のデータの一部をマスクしたり、並べ替えたりして正解を予想させることで、対象となるデータの特徴をうまく表現する情報を獲得することができます(表現学習)。
\item[敵対的生成ネットワーク]  \mbox{} \\
  生成モデルと識別モデルを組み合わせ、互いを競わせるように学習させることで、現実には存在しない画像などを生成する手法が提案され、広く使われています。
\end{description}

\subsection{環境}

\begin{description}
\item[データサイエンティストブーム] \mbox{} \\
  2000年代よりデータサイエンティストブームが発生し、ゼロ年代は統計的手法が広まりました。
  その流れは2010年台でも継続し、深層学習手法にも波及しています。
  数多くのMOOCsプラットフォームも立ち上がり、安価で良質な学習環境が整備されています。
\item[コンペティションの隆盛] \mbox{} \\
  Kaggle, SIGNATEといったコンペティションサイトが立ち上がり、技術者の研鑽が日々なされています。
\item[実行環境の充実] \mbox{} \\
  かねてから GPGPU 環境は AWS, GCE などで提供されていましたが、それ以外にも無料でGPUの利用が可能なGoogle Colaboratoryといったサービスが使えるようになり、ブラウザさえあれば環境構築不要で機械学習が行えるようになりました。
\end{description}

\subsection{もたらされた変化}

\begin{description}
\item[人を越える精度/能力] \mbox{} \\
  画像認識分野では、ImageNet(\url{http://image-net.org/})というデータセットでいかに高い認識精度を達成するか、という試みが行われています。2012年にディープラーニングベースの手法 AlexNet が登場して以来、急速にその精度は向上し、2015年には ResNet によって人間の認識能力を超える精度を達成しました(参考: \url{https://www.slideshare.net/ren4yu/ss-234439652/21})。\\
  自然言語の分野でも、Stanford Question Answering Dataset (SQuAD) (\url{https://rajpurkar.github.io/SQuAD-explorer/})などで人間の正答率を超える正答率を達成しています。\\
  問題を分類タスクに落とし込むことができ、十分な学習データを用意することができれば、機械学習モデルは人間を超える精度を達成できる可能性を示しています。
\item[生成モデル] \mbox{} \\
  たくさんの画像やテキストデータ、音楽データから、新しいデータを生成するモデルが広く使われるようになりました。\\
  主に敵対的生成ネットワークによる成果ですが、テキストに関しては言語モデルベースの手法(GPT-2等)が主流です。\\
  さらにこれらを応用することで、画像復元(inpainting)や自動着色、さらには描画支援(GauGAN)といったことが実現できています。
\item[機械翻訳] \mbox{} \\
  ディープラーニング以前の機械翻訳は統計ベースの手法が利用されていました。この手法はあまり流暢ではない文が生成されがちという欠点があったのですが、ニューラル機械翻訳によって、その問題は大きく改善されました。\\
  一方でGPUが持つメモリがボトルネックとなり、語彙を多くもたせることが難しくなりました。そのため、単語より小さな単位(サブワード)でトークンを区切ることによって語彙を抑える手法が主流となってきています。\\
  ニューラルネットワークの学習は多くがend-to-endで行えるため、正確な単語区切りや品詞を考慮しなくても良くなったという事情もあります。\\
  その副作用として、多言語を一度に扱えるようにもなりました。サブワードより小さい、バイト単位で区切る手法もあります。この場合、デコード時に正しいバイト列であることを保証する必要性があります。
\end{description}

\subsection{新たな課題}

ニューラルネットワーク固有の課題も登場しています。

\begin{description}
\item[翻訳ミス] \mbox{} \\
  統計ベースの機械翻訳でも翻訳ミスはありましたが、ニューラル機械翻訳の場合、それを見つけることが難しくなっています。なまじ流暢な結果を出力してしまうため、注意深く見ないと訳抜けなどを見逃してしまいがちです。
  また、統計ベースでは、原文と翻訳文のフレーズ対応が処理の関係上あきらかだったのですが、ニューラル機械翻訳ではend-to-endな学習になったことで、翻訳文と原文との対応づけが明らかでなくなったというデメリットもあります。\\
  他に、逆の意味を出力してしまうことがある問題もあります。これも訳文の対応付けが不明瞭なため、原因をさぐることは難しい問題です。\\
  同じ文言を繰り返し生成するという、ニューラル機械翻訳特有の問題もあります(参考事例: \url{https://www.slideshare.net/ToshiakiNakazawa/nlp2017-nmt-tutorial/79})。
\item[データセットとバイアス] \mbox{} \\
  近年よく問題としてとりあげられるものに、バイアスがあります。これは、学習に使ったデータセット内に偏りがあった場合、特に注意をしないとそれがそのまま機械学習モデルに反映されてしまいます。\\
  たとえば、多くの場合英語で''sister''という単語は姉・妹の区別をつけることはあまりなく、機械翻訳でそれを日本語にした際に、文脈に応じて出現しやすい方に訳してしまうことが発生します。
\item[敵対的サンプル] \mbox{} \\
  人間が見るとパンダなのに、識別モデルにはテナガザルに見えるような画像が有名です(参考: \url{https://speakerdeck.com/settenqb/ji-jie-xue-xi-tosekiyuritei?slide=5})。このように機械翻訳モデルを騙す画像を生成する技術と、それを防衛する技術が互いにしのぎを削っています。
\item[バックドア] \mbox{} \\
  訓練に用いるデータセットに悪意あるデータを混ぜることで、バックドアを仕掛けるという手法もあります。クラウドソーシングなどを使う際には、用途によってはそのような問題があることを想定しておく必要もあるでしょう。
\end{description}

\subsection{フレームワーク}

よく使わていた(いる)代表的なフレームワークには以下のものがあります。

\begin{itemize}
\item 古参
  \begin{itemize}
  \item Torch7
  \item Caffe
  \item Theano
  \end{itemize}
\item 現代的なもの
  \begin{itemize}
  \item Chainer
  \item PyTorch
  \item TensorFlow
  \item Keras
  \item Darknet
  \end{itemize}
\end{itemize}

どれも多次元配列を扱うことができ、自動微分機能を備えています。

\subsection{Web/JavaScriptランタイム}

先に述べたようなフレームワークで訓練されたモデルを、ブラウザやJavaScriptで実行するフレームワークがいくつかあります。

\begin{itemize}
\item WebDNN
  \begin{itemize}
  \item https://mil-tokyo.github.io/webdnn/ja/
  \item 日本発
  \item 複数のフレームワークに対応
  \item 現在開発はinactive
  \end{itemize}
\item TensorFlow.js
  \begin{itemize}
  \item https://www.tensorflow.org/js
  \item TensorFlow公式
  \item 訓練もサポート
  \item 多くの訓練済みモデルを配布
  \item https://github.com/tensorflow/tfjs-models
  \end{itemize}
\item ONNX.js
  \begin{itemize}
  \item https://github.com/microsoft/onnxjs
  \item フレームワーク共通フォーマットONNXに対応
  \item 対応している演算に制限あり
  \end{itemize}
\end{itemize}

\subsection{作成したものの紹介}

WebDNNを用いて、いくつかのデモサイトを公開しています。

\begin{description}
\item[edges2cats] \mbox{} \\
  Chainerとpix2pixを用いて、猫の線画に着色をするモデルを公開しています (\url{http://pix2pix.daio.net/})。
  公式のデモサイト \url{https://affinelayer.com/pixsrv/} でも同じことができますが、こちらは猫着色モデルのみ、訓練データを公開していません。それ以外のデモは訓練データを含め公開しているので再現可能です。\\
  同じことをするために、猫画像収集スクリプトを公開しています \url{https://github.com/knok/pixabay-cat-images}。\\
  猫画像から背景の除去は DilatedNetのkeras実装 \url{https://github.com/nicolov/segmentation_keras} とその訓練済みモデルを用いています (解説記事: \url{https://qiita.com/knok/items/6ad09cc870739dbd921b})。
\item[Wasserstein AutoEncoder] \mbox{} \\
  TensorFlowで実装されていた著者実装 \url{https://github.com/tolstikhin/wae} をもとに、Chainerへ移植し、2次元の潜在空間へ300程度のイラストを埋め込み、推論を行えるサイトとして公開しました \url{http://wae-friends.daio.net} 。
\end{description}

また、TensorFlow.jsと訓練済みモデルを利用したデモサイトも公開しています。

\begin{description}
\item[バーチャル背景] \mbox{} \\
  訓練済みBodyPixモデル \url{https://github.com/tensorflow/tfjs-models/tree/master/body-pix}を用いてWebカムから人体部分のみを抽出(セマンティックセグメンテーション)し、背景用画像と合成して表示するサイトをGitHub Pagesで公開しています \url{https://knok.github.io/virtbg/} (ソースコード: \url{https://github.com/knok/virtbg})。\\
  これ単体では配信などには使えませんが、OBS Studioのウィンドウキャプチャー機能とOBS-v4l2sinkを組み合わせれば、v4l2loopbackを利用可能な任意のサービスに応用できます(解説記事: \url{https://qiita.com/knok/items/b3eb87769151ac04efeb})。\\
\item[VRM Thee.js PoseNet Sample] \mbox{} \\
  既存の公開されていたコード \url{https://github.com/t-takasaka/vrm-three-posenet} をforkし、最近のAPIに合わせつつ再配布可能なVRMモデルに差し替えたものを公開しています \url{https://knok.github.io/vrm-three-posenet/} (ソースコード: \url{https://github.com/knok/vrm-three-posenet})。\\
  こちらは訓練済みPoseNet \url{https://github.com/tensorflow/tfjs-models/tree/master/posenet} を利用しています。
\end{description}

\subsection{Debianにおける訓練済みモデルの扱い}

2020年7月時点では、ニューラルネットワークの訓練済みモデルについて、まだ明確なポリシーは決まっていません。ドラフトレベルのものはあります \url{https://salsa.debian.org/deeplearning-team/ml-policy}。
再現性のため、訓練データや訓練用コードを含んだ状態が望ましいという方向性で進みつつあります。\\
その観点でいうと、TensorFlow Hubやtfjs-modelsはその条件を満たしていません。そういったものを用いるツールはおそらくmainとして配布されないのではないかと個人的には思っています。\\
一方で、Deepでない機械学習訓練済みモデルは既に配布されている現状があります (例: OpenCVの識別モデル \url{https://salsa.debian.org/science-team/opencv/-/tree/master/data})。
機械学習という分野自体でも再現性は重視されてはいますが、公開されている多くのものは研究用途に限定したデータセットや訓練済みモデルなのが現状です。
プライバシーとの兼ね合いもあり、この問題の解決は難しいと思われます。

\subsection{まとめ}

2010年の勉強会から10年が経過し、ニューラルネットワークをベースとした機械学習手法は飛躍的な発展を遂げました。
今ではWebブラウザやスマートフォンなどのモバイルデバイスで動作させることも容易になってきています。

一方でニューラルネットワーク固有の問題や、データセットに起因する諸問題(バイアス、プライバシー等)もあります。

DFSGに照らし合わせると、再現性を担保するようなデータセットや訓練コードを伴った訓練済みモデルの配布が望ましいところですが、その実現はなかなかむずかしそうです。


% 冊子にするために、4の倍数にする必要がある。
% そのための調整
\dancersection{メモ}{}
\mbox{}\newpage

\vspace*{15cm}
\hrule
\vspace{2mm}
\includegraphics[width=2cm]{image200502/openlogo-nd.eps}
\noindent \Large \bf Debian 勉強会資料\\
\noindent \normalfont \debmtgyear{}年\debmtgmonth{}月\debmtgdate{}日 \hspace{5mm}  初版第1刷発行\\
\noindent \normalfont 東京エリア Debian 勉強会 （編集・印刷・発行）\\
\hrule
\end{document}
