%; whizzy chapter -dvi
% -initex iniptex -latex platex -format platex -bibtex jbibtex -fmt fmt
% 以上 whizzytex を使用する場合の設定。

%     Tokyo Debian Meeting resources
%     Copyright (C) 2012 Junichi Uekawa
%     Copyright (C) 2011, 2015, 2020 Nobuhiro Iwamatsu

%     This program is free software; you can redistribute it and/or modify
%     it under the terms of the GNU General Public License as published by
%     the Free Software Foundation; either version 2 of the License, or
%     (at your option) any later version.

%     This program is distributed in the hope that it will be useful,
%     but WITHOUT ANY WARRANTY; without even the implied warranty of
%     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%     GNU General Public License for more details.

%     You should have received a copy of the GNU General Public License
%     along with this program; if not, write to the Free Software
%     Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

%  preview (shell-command (concat "evince " (replace-regexp-in-string "tex$" "pdf"(buffer-file-name)) "&"))

%%ここからヘッダ開始。

\documentclass[mingoth,a4paper]{jsarticle}
\usepackage{monthlyreport}
% 日付を定義する、毎月変わります。
\newcommand{\debmtgyear}{2020}
\newcommand{\debmtgmonth}{7}
\newcommand{\debmtgdate}{18}
% started from zero:
% (let ((year 2013) (month 7)) (+ (* (- year 2005) 12) month -1))
\newcommand{\debmtgnumber}{17}

% Needed to import pandoc-generated LaTeX documents.
% See https://stackoverflow.com/questions/40438037/tightlist-error-using-pandoc-with-markdown
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% tikz picture の為のマクロ設定
\usepackage[dvipdfmx]{graphicx}
\usepackage{tikz}

\begin{document}

\begin{titlepage}
\thispagestyle{empty}
% タイトルページ:編集必要な部分は最初のマクロに飛ばすこと

\vspace*{-2cm}
第\debmtgnumber{}回 東京エリア Debian 勉強会資料\\
\hspace*{-2cm}
\includegraphics{image2012-natsu/dotdeb.pdf}\\
\hfill{}\debmtgyear{}年\debmtgmonth{}月\debmtgdate{}日

% ここはアップデートすること
% 全角文字にしないとフォントのサイズが合わないので注意
\rotatebox{10}{\fontsize{30}{30} {\gt　ニューラルネットワーク特集}}\\

\vspace*{-2cm}
\hfill{}\includegraphics[height=6cm]{image200502/openlogo-nd.eps}
\end{titlepage}

\newpage

\begin{minipage}[b]{0.2\hsize}
 \definecolor{titleback}{gray}{0.9}
 \colorbox{titleback}{\rotatebox{90}{\fontsize{80}{80} {\gt デビアン勉強会} }}
\end{minipage}
\begin{minipage}[b]{0.8\hsize}
\hrule
\vspace{2mm}
\hrule
\begin{multicols}{2}
\tableofcontents
\end{multicols}
\vspace{2mm}
\hrule
\end{minipage}

\dancersection{最近のDebian関連のミーティング報告}{杉本　典充}

\subsection{2020年6月度 東京エリア・関西合同Debian勉強会}

2020年6月29日(土)に東京エリアDebian勉強会と関西Debian勉強会の合同でオンラインによるDebian勉強会を開催しました。参加者は12名でした。


\dancersection{事前課題}{杉本　典充}

今回の事前課題は以下です。

\begin{enumerate}
\item xxx
\end{enumerate}

%この課題に対して提出いただいた内容は以下です。

\begin{multicols}{2}
{\small
\input{image202007/prework.tex}
}
\end{multicols}

%\dancersection{Debian Trivia Quiz}{username}
%
%Debianの昨今の話題についてのQuizです。
%
%今回の出題範囲は\url{debian-devel-announce@lists.debian.org} や \url{debian-news@lists.debian.org}などに投稿された内容からです。
%
%\begin{multicols}{2}
%\input{image202007/quiz.tex}
%\end{multicols}


% % (query-replace-regexp "<.*?>" "")
% % (query-replace-regexp "^[    ]\+" "")

%\dancersection{titme}{username}
%-------------------------------------------------------------------------------

\dancersection{10年ぶりのニューラルネットワーク}{野首貴嗣}

過去、2010年3月に行われた第62回東京エリアDebian勉強会では
「ニューラルネットワークで画像認識してみた」
というタイトルで本庄弘典さんが発表をされていました。

それから10年が経過した2020年7月現在、ニューラルネットワークをとりまく
状況がどのように変化したかを俯瞰的に見ていきます。

また、個人的に開発、公開している成果につても紹介します。

\subsectin{ニューラルネットワークのお気持ち}

まず、ニューラルネットワークについての厳密ではない、なんとなくなお気持ちで
概要を解説します。

パーセプトロン(\url{https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3})と呼ばれる、人間の脳(ニューロン)を模した演算ユニットがあります。
これは入力信号Xに対し、重みWを乗じバイアスbを加算した結果Yをえる単純なユニットです。さらに、Yに対し活性化関数と呼ばれる関数を適用しその結果を分類タスクの結果として考えます。

分類が正しいなら1を、間違っているなら0を返す関数と考え、実際に用意したデータをもちいて訓練を行います。訓練は、逆誤差伝播(バックプロパゲーション)という手法を使い、求められた結果と訓練データ上の正解との誤差を最小化するよう、重みを調整していきます。

単一のパーセプトロンはANDやORを表現することができます。パーセプトロンの式は線形分離境界を表していると考えることで直感的に理解できるでしょう。
さらには、XORは実現できないことも理解できると思います(1つの直線で分離できないため)。

ニューロンを増やしたり、多段に接続することで、より複雑な非線形分離が可能となります。この段数が概ね4段以上になったものをディープラーニングと呼ぶことが多いようです。

\subsection{ブレイクスルー}

2012年に、Googleが''Using large-scale brain simulations for machine learning and A.I.''(\url{https://blog.google/topics/machine-learning/using-large-scale-brain-simulations-for/})というblog記事と論文
(\url{http://static.googleusercontent.com/media/research.google.com/ja//archive/unsupervised_icml2012.pdf})
を公開しました。
16コアをもつマシン169台を駆使し、大量の画像を訓練することで教師なしで人体や猫の特徴を獲得したと話題になりました。このときに採用されたアーキテクチャはAutoAEncoderと呼ばれるものです。

AutoEncoderは入力側と出力側が等しくなるように構成された多段ニューラルネットワークで、入出力画像よりも小さなベクトル表現によって、入力画像の特徴を獲得することができるものです。
AutoEncoderが行っていることは、一種の次元圧縮ともいえます。ニューラルネットワークを用いない次元圧縮手法もありますが、多段ニューラルネットワーク(ディープラーニング)が実用になることを示した大きな一歩でした。

\subsection{ブレイクスルー実現要素}

従来のニューラルネットワークが抱えていた課題を解決した要素にはさまざまなものがあります。

\begin{itemize}
\item 計算資源の向上
  私がニューラルネットワークに触れた1995年の時点で、実際に利用されていたコンピュータのスペックは次のようなものでした。
  \begin{itemize}
  \item PA-RISC 25Mhz
  \item メモリ16MB
  \end{itemize}
  そもそも利用可能なメモリが少なく、大きなニューラルネットワーク構造を持つこと自体が困難でした。現代においてはスマートフォンでも4GB程度のメモリを搭載するものも珍しくありません。
\item GPGPUの一般化
  CPU自体も当時と比較してかなり高速にはなりましたが、単一コアあたりの処理能力向上には限界が見えつつあります。一方でグラフィックス処理向けに設計されたGPUをより一般的なコンピューティングに応用する環境が整備されてきました。
  GPUは特に並列計算を得意としており、多段ニューラルネットワークの演算とは非常に相性がよく、その方向へ進化していっています。
\item 大規模データセットの拡充
  Googleの猫の例で活躍したのは、Google photosやFacebook album等、画像データを大量に収集できるプラットフォームの存在でした。
  さらに、ラベル(物体の種別情報)を付与したデータセットも増えていきます。これにはクラウドソーシングサービスの拡充(Amazon Mechanical Turk, Cloudworks等)も寄与しています。
\item 技術的な進歩
  多段ニューラルネットワークが抱えていた他の問題として、勾配消失/発散問題がありました。逆誤差伝播をする際に、層が増えることによって伝播すべき誤差がどんどん小さくなってしまったり、逆に大きくなったりする問題です。
  要因の一つに、活性化関数があります。シグモイド関数を使うことが昔は一般的でしたが、この関数が取りうる値は0〜1しかありません。より幅広い値を扱える活性化関数(ReLU等のランプ関数)の出現により、より大きな勾配を扱えるようになりました。
  また勾配のクリッピングによって、勾配爆発を抑えることができるようになりました。
  その他Batch Normalizationなどの正規化手法も学習の安定化に寄与しています。
\item 分散表現の活用
  単語や抽象的なカテゴリーといったものを表現するために分散表現(埋め込み表現)が活用できることがわかり、自然言語処理などへの活用が広がりました。word2vecは単語埋め込み表現の最たる活用例です。
\item データ拡張
  既存のデータを変形したり、ノイズをのせるなどのデータ拡張技術が主に画像認識分野で発達しています。これにより、学習データを水増しすることができ、認識制度向上に寄与しています。
\item 教師データを必要としない学習
  一般的な教師あり学習には正解データを用意する必要があり、その作成にはコストがかかります。一方でword2vecのような、教師データを必要としない手法が発達してきています。
  既存のデータの一部をマスクしたり、並べ替えたりして正解を予想させることで、対象となるデータの特徴をうまく表現する情報を獲得することができます(表現学習)。
\item 敵対的生成ネットワーク
  生成モデルと識別モデルを組み合わせ、互いを競わせるように学習させることで、現実には存在しない画像などを生成する手法が提案され、広く使われています。
\end{itemize}

\subsection{環境}

\begin{itemize}
\item データサイエンティストブーム
  2000年代よりデータサイエンティストブームが発生し、ゼロ年代は統計的手法が広まりました。
  その流れは2010年台でも継続し、深層学習手法にも波及しています。
  数多くのMOOCsプラットフォームも立ち上がり、安価で良質な学習環境が整備されています。
\item コンペティションの隆盛
  Kaggle, SIGNATEといったコンペティションサイトが立ち上がり、技術者の研鑽が日々なされています。
\item 実行環境の充実
  かねてからGPGPU環境はAWS, GCEなどで提供されていましたが、それ以外にも無料でGPUの利用が可能なGoogle Colaboratoryといったサービスが使えるようになり、ブラウザさえあれば環境構築不要で機械学習が行えるようになりました。
\end{itemize}

% 冊子にするために、4の倍数にする必要がある。
% そのための調整
\dancersection{メモ}{}
\mbox{}\newpage

\vspace*{15cm}
\hrule
\vspace{2mm}
\includegraphics[width=2cm]{image200502/openlogo-nd.eps}
\noindent \Large \bf Debian 勉強会資料\\
\noindent \normalfont \debmtgyear{}年\debmtgmonth{}月\debmtgdate{}日 \hspace{5mm}  初版第1刷発行\\
\noindent \normalfont 東京エリア Debian 勉強会 （編集・印刷・発行）\\
\hrule
\end{document}
